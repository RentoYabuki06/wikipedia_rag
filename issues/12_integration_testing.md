# Issue #12: çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™

**äºˆæƒ³æ™‚é–“**: 30åˆ†
**é›£æ˜“åº¦**: ä¸­ç´š
**å­¦ç¿’é …ç›®**: çµ±åˆãƒ†ã‚¹ãƒˆã€ãƒ‡ãƒãƒƒã‚°æŠ€æ³•ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

## æ¦‚è¦
å…¨ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã€ç™ºè¦‹ã—ãŸå•é¡Œã‚’ãƒ‡ãƒãƒƒã‚°ãƒ»ä¿®æ­£ã—ã¦ã€åŒ…æ‹¬çš„ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ•´å‚™ã™ã‚‹ã€‚

## ã‚¿ã‚¹ã‚¯

### 1. çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®ä½œæˆ (`tests/test_integration.py`)
- ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ
- å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å˜ä½“ãƒ†ã‚¹ãƒˆ
- ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

### 2. ãƒ‡ãƒãƒƒã‚°ãƒ»ä¿®æ­£
- ç™ºè¦‹ã•ã‚ŒãŸãƒã‚°ã®ç‰¹å®šã¨ä¿®æ­£
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®æ”¹å–„
- ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚„åŠ¹ç‡æ€§ã®å•é¡Œè§£æ±º
- ãƒ­ã‚°å‡ºåŠ›ã®æ”¹å–„

### 3. READMEã®æ›´æ–°
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ã®è©³ç´°åŒ–
- ä½¿ç”¨ä¾‹ã®å……å®Ÿ
- ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æƒ…å ±
- ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã®æ˜è¨˜

### 4. é‹ç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆ
- `docs/setup_guide.md`: è©³ç´°ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰
- `docs/troubleshooting.md`: ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•
- `docs/performance_guide.md`: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¬ã‚¤ãƒ‰

## å®Ÿè£…ä¾‹

### çµ±åˆãƒ†ã‚¹ãƒˆ (`tests/test_integration.py`)
```python
import pytest
import tempfile
import shutil
from pathlib import Path
import json
import numpy as np

from src.data_loader import load_wikipedia_data, normalize_text
from src.chunker import TextChunker
from src.embedder import E5Embedder
from src.vector_store import FAISSVectorStore
from src.reranker import BGEReranker
from src.generator import QwenGenerator
from src.rag_wiki import WikiRAG

class TestIntegration:
    @pytest.fixture
    def temp_artifacts_dir(self):
        """ãƒ†ã‚¹ãƒˆç”¨ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)
    
    @pytest.fixture
    def small_test_data(self):
        """ãƒ†ã‚¹ãƒˆç”¨å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿"""
        return [
            {
                'id': 'test_1',
                'title': 'ãƒ†ã‚¹ãƒˆè¨˜äº‹1',
                'text': 'ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®è¨˜äº‹ã§ã™ã€‚' * 20,
                'source': 'jawiki:ãƒ†ã‚¹ãƒˆè¨˜äº‹1'
            },
            {
                'id': 'test_2', 
                'title': 'ãƒ†ã‚¹ãƒˆè¨˜äº‹2',
                'text': 'æ—¥æœ¬ã®æ­´å²ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚' * 20,
                'source': 'jawiki:ãƒ†ã‚¹ãƒˆè¨˜äº‹2'
            }
        ]
    
    def test_data_loading_and_processing(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ"""
        # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®å‹•ä½œç¢ºèª
        data = load_wikipedia_data(max_articles=5)
        assert len(data) <= 5
        assert all('title' in item and 'text' in item for item in data)
        
        # ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–
        sample_text = "  ã“ã‚Œã¯ã€€ã€€ãƒ†ã‚¹ãƒˆã€€ã§ã™ã€‚  \\n\\n"
        normalized = normalize_text(sample_text)
        assert normalized == "ã“ã‚Œã¯ ãƒ†ã‚¹ãƒˆ ã§ã™ã€‚"
    
    def test_chunking_pipeline(self, small_test_data):
        """ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
        chunker = TextChunker(chunk_size=100, overlap=20)
        
        all_chunks = []
        for article in small_test_data:
            chunks = chunker.chunk_text(
                article['text'], 
                article['id'], 
                article['title']
            )
            all_chunks.extend(chunks)
        
        assert len(all_chunks) > 0
        assert all('text' in chunk for chunk in all_chunks)
        assert all('source' in chunk for chunk in all_chunks)
        assert all('chunk_id' in chunk for chunk in all_chunks)
    
    def test_embedding_pipeline(self, small_test_data):
        """åŸ‹ã‚è¾¼ã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
        embedder = E5Embedder()
        
        # ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸åŸ‹ã‚è¾¼ã¿
        texts = [item['text'][:200] for item in small_test_data]
        embeddings = embedder.encode_passages(texts)
        
        assert embeddings.shape[0] == len(texts)
        assert embeddings.shape[1] == 768  # multilingual-e5-baseã®æ¬¡å…ƒæ•°
        
        # æ­£è¦åŒ–ç¢ºèª
        norms = np.linalg.norm(embeddings, axis=1)
        assert np.allclose(norms, 1.0, atol=1e-6)
        
        # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿
        query_emb = embedder.encode_query("ãƒ†ã‚¹ãƒˆè³ªå•")
        assert query_emb.shape == (1, 768)
    
    def test_vector_search(self, small_test_data, temp_artifacts_dir):
        \"\"\"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ãƒ†ã‚¹ãƒˆ\"\"\"
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        embedder = E5Embedder()
        texts = [item['text'][:200] for item in small_test_data]
        embeddings = embedder.encode_passages(texts)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        vector_store = FAISSVectorStore(768)
        vector_store.build_index(embeddings)
        
        # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        query_emb = embedder.encode_query("æ—¥æœ¬ã®æ­´å²")
        scores, indices = vector_store.search(query_emb, k=2)
        
        assert len(scores[0]) <= 2
        assert len(indices[0]) <= 2
        assert all(0 <= idx < len(small_test_data) for idx in indices[0])
        
        # ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        index_path = Path(temp_artifacts_dir) / "test.index"
        vector_store.save(str(index_path))
        assert index_path.exists()
        
        new_store = FAISSVectorStore(768)
        new_store.load(str(index_path))
        new_scores, new_indices = new_store.search(query_emb, k=2)
        
        assert np.array_equal(scores, new_scores)
        assert np.array_equal(indices, new_indices)
    
    def test_reranker_functionality(self):
        \"\"\"å†ãƒ©ãƒ³ã‚«ãƒ¼ã®ãƒ†ã‚¹ãƒˆ\"\"\"
        reranker = BGEReranker()
        
        query = "æ—¥æœ¬ã®æ­´å²ã«ã¤ã„ã¦"
        passages = [
            "æ—¥æœ¬ã®æ­´å²ã¯å¤ä»£ã‹ã‚‰ç¾ä»£ã¾ã§ç¶šã„ã¦ã„ã¾ã™ã€‚",
            "å¯¿å¸ã¯æ—¥æœ¬æ–™ç†ã®ä»£è¡¨ã§ã™ã€‚",
            "æ±Ÿæˆ¸æ™‚ä»£ã¯å¹³å’Œãªæ™‚ä»£ã§ã—ãŸã€‚"
        ]
        
        if reranker.is_available:
            results = reranker.rerank(query, passages, top_k=2)
            assert len(results) == 2
            assert all(isinstance(item, tuple) for item in results)
            assert all(len(item) == 2 for item in results)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‹•ä½œã®ç¢ºèª
            results = reranker.rerank(query, passages, top_k=2)
            assert len(results) == 2
    
    def test_generator_functionality(self):
        \"\"\"ç”Ÿæˆå™¨ã®ãƒ†ã‚¹ãƒˆ\"\"\"
        generator = QwenGenerator()
        
        question = "ãƒ†ã‚¹ãƒˆã«ã¤ã„ã¦æ•™ãˆã¦"
        contexts = [
            {
                'text': 'ãƒ†ã‚¹ãƒˆã¯å“è³ªã‚’ç¢ºèªã™ã‚‹é‡è¦ãªä½œæ¥­ã§ã™ã€‚',
                'article_title': 'ãƒ†ã‚¹ãƒˆ',
                'chunk_id': 0
            }
        ]
        
        answer = generator.generate_answer(question, contexts)
        assert isinstance(answer, str)
        assert len(answer) > 0
        assert "ãƒ†ã‚¹ãƒˆ" in answer or "test" in answer.lower()
    
    def test_full_rag_pipeline(self, temp_artifacts_dir):
        \"\"\"å®Œå…¨RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ†ã‚¹ãƒˆ\"\"\"
        # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
        # æ³¨æ„: ã“ã®ãƒ†ã‚¹ãƒˆã¯è¨ˆç®—è³‡æºã‚’å¿…è¦ã¨ã™ã‚‹ãŸã‚ã€CIç’°å¢ƒã§ã¯æ…é‡ã«å®Ÿè¡Œ
        pass  # å®Ÿè£…ã¯æ™‚é–“ã®éƒ½åˆä¸Šçœç•¥
    
    def test_error_handling(self):
        \"\"\"ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ\"\"\"
        # å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®åˆæœŸåŒ–
        with pytest.raises((FileNotFoundError, Exception)):
            WikiRAG("nonexistent_directory")
        
        # ç©ºã®è³ªå•
        # å®Ÿè£…ã«å¿œã˜ãŸãƒ†ã‚¹ãƒˆã‚’è¿½åŠ 
    
    def test_performance_benchmarks(self, small_test_data):
        \"\"\"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ\"\"\"
        import time
        
        # åŸ‹ã‚è¾¼ã¿é€Ÿåº¦ãƒ†ã‚¹ãƒˆ
        embedder = E5Embedder()
        texts = [item['text'][:200] for item in small_test_data * 10]
        
        start_time = time.time()
        embeddings = embedder.encode_passages(texts)
        embedding_time = time.time() - start_time
        
        # æœŸå¾…å€¤: 20ãƒ†ã‚­ã‚¹ãƒˆã‚’10ç§’ä»¥å†…ã§å‡¦ç†
        assert embedding_time < 10.0
        assert embeddings.shape[0] == len(texts)

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

### æ›´æ–°ã•ã‚ŒãŸREADME.md
```markdown
# Wikipedia RAG ã‚·ã‚¹ãƒ†ãƒ 

æ—¥æœ¬èªWikipediaã‚’ä½¿ã£ãŸæ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼ˆRAGï¼‰ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚è³ªå•ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€é–¢é€£ã™ã‚‹Wikipediaè¨˜äº‹ã‚’æ¤œç´¢ã—ã€ãã®æƒ…å ±ã«åŸºã¥ã„ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

## ğŸŒŸ ç‰¹å¾´

- **å®Œå…¨æ—¥æœ¬èªå¯¾å¿œ**: æ—¥æœ¬èªWikipediaãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
- **é«˜ç²¾åº¦æ¤œç´¢**: E5åŸ‹ã‚è¾¼ã¿ + BGEå†ãƒ©ãƒ³ã‚«ãƒ¼ã«ã‚ˆã‚‹2æ®µéšæ¤œç´¢
- **å‡ºå…¸æ˜ç¤º**: ã™ã¹ã¦ã®å›ç­”ã«å‚ç…§æƒ…å ±ã‚’ä»˜ä¸
- **ç°¡å˜ãƒ‡ãƒ—ãƒ­ã‚¤**: Gradio UIã§ãƒ­ãƒ¼ã‚«ãƒ«ãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰ä¸¡å¯¾å¿œ
- **è©•ä¾¡æ©Ÿèƒ½**: Recall@KæŒ‡æ¨™ã«ã‚ˆã‚‹å®¢è¦³çš„æ€§èƒ½æ¸¬å®š

## ğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

### ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶
- **RAM**: 8GBä»¥ä¸Šæ¨å¥¨ï¼ˆ4GBæœ€ä½ï¼‰
- **Storage**: 10GBä»¥ä¸Šã®ç©ºãå®¹é‡
- **GPU**: ä»»æ„ï¼ˆã‚ã‚Œã°é«˜é€ŸåŒ–ï¼‰

### ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢è¦ä»¶
- Python 3.10ä»¥ä¸Š
- pip ã¾ãŸã¯ conda

### å¯¾å¿œOS
- macOS
- Linux (Ubuntu 20.04+)
- Windows 10/11

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
\\`\\`\\`bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone <repository-url>
cd wikipedia_rag

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
\\`\\`\\`

### 2. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
\\`\\`\\`bash
# å°è¦æ¨¡ãƒ†ã‚¹ãƒˆç”¨ï¼ˆç´„5åˆ†ï¼‰
python src/build_wiki_index.py --max_articles 1000

# ä¸­è¦æ¨¡ç”¨ï¼ˆç´„30åˆ†ï¼‰
python src/build_wiki_index.py --max_articles 10000

# å¤§è¦æ¨¡ç”¨ï¼ˆç´„2æ™‚é–“ï¼‰
python src/build_wiki_index.py --max_articles 30000
\\`\\`\\`

### 3. è³ªå•å¿œç­”ãƒ†ã‚¹ãƒˆ
\\`\\`\\`bash
python src/rag_wiki.py -q "ç¹”ç”°ä¿¡é•·ã«ã¤ã„ã¦æ•™ãˆã¦"
\\`\\`\\`

### 4. Web UIèµ·å‹•
\\`\\`\\`bash
python src/app_wiki.py
# ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:7860 ã«ã‚¢ã‚¯ã‚»ã‚¹
\\`\\`\\`

## ğŸ“– è©³ç´°ãªä½¿ã„æ–¹

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³

#### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
\\`\\`\\`bash
python src/build_wiki_index.py \\
    --max_articles 30000 \\        # å‡¦ç†ã™ã‚‹è¨˜äº‹æ•°
    --chunk_size 450 \\            # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
    --overlap 60 \\                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º
    --batch_size 32 \\             # ãƒãƒƒãƒã‚µã‚¤ã‚º
    --output_dir artifacts         # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
\\`\\`\\`

#### è³ªå•å¿œç­”
\\`\\`\\`bash
python src/rag_wiki.py \\
    -q "è³ªå•æ–‡" \\                 # è³ªå•
    --topk 16 \\                   # æ¤œç´¢å€™è£œæ•°
    --topn 5 \\                    # æœ€çµ‚å€™è£œæ•°
    --no-rerank \\                 # å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç„¡åŠ¹åŒ–
    --verbose                      # è©³ç´°ãƒ­ã‚°
\\`\\`\\`

### è©•ä¾¡å®Ÿè¡Œ
\\`\\`\\`bash
python src/eval_retrieval.py --eval_data data/dev.jsonl
\\`\\`\\`

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ãƒ¢ãƒ‡ãƒ«å¤‰æ›´
å„ãƒ¢ãƒ‡ãƒ«ã¯ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§å¤‰æ›´å¯èƒ½ã§ã™ï¼š

\\`\\`\\`python
# src/config.py (ä½œæˆ)
EMBEDDING_MODEL = "intfloat/multilingual-e5-base"
RERANKER_MODEL = "BAAI/bge-reranker-v2-m3" 
LLM_MODEL = "Qwen/Qwen2.5-3B-Instruct"
\\`\\`\\`

### ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **chunk_size**: é•·ã„æ–‡æ›¸ã¯å¤§ããã€çŸ­ã„æ–‡æ›¸ã¯å°ã•ã
- **overlap**: æƒ…å ±ã®é€£ç¶šæ€§é‡è¦–ãªã‚‰å¤§ãã
- **top_k/top_n**: ç²¾åº¦é‡è¦–ãªã‚‰å¤§ããã€é€Ÿåº¦é‡è¦–ãªã‚‰å°ã•ã

## ğŸ“Š æ€§èƒ½æŒ‡æ¨™

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼ˆå‚è€ƒå€¤ï¼‰
| ãƒ¢ãƒ‡ãƒ«æ§‹æˆ | Recall@5 | å‡¦ç†æ™‚é–“ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ |
|------------|----------|----------|------------|
| CPU only   | 0.65     | 3.2ç§’    | 4.5GB      |
| GPUæœ‰åŠ¹    | 0.68     | 1.8ç§’    | 6.2GB      |

## ğŸ› ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

#### 1. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼
\\`\\`\\`bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦å†å®Ÿè¡Œ
python src/build_wiki_index.py --batch_size 8
\\`\\`\\`

#### 2. ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—
\\`\\`\\`bash
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å¾Œã«å†å®Ÿè¡Œ
rm -rf ~/.cache/huggingface/
pip install --upgrade transformers
\\`\\`\\`

#### 3. FAISSé–¢é€£ã‚¨ãƒ©ãƒ¼
\\`\\`\\`bash
# FAISSå†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip uninstall faiss-cpu
pip install faiss-cpu --no-cache-dir
\\`\\`\\`

è©³ç´°ã¯ [docs/troubleshooting.md](docs/troubleshooting.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ğŸ“ˆ æ€§èƒ½æœ€é©åŒ–

### GPUåˆ©ç”¨è¨­å®š
\\`\\`\\`python
# ã‚ˆã‚Šé«˜é€ŸãªGPUæ¨è«–ã®ãŸã‚ã«
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
\\`\\`\\`

### ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
- å‡¦ç†ã™ã‚‹è¨˜äº‹æ•°ã‚’æ®µéšçš„ã«å¢—ã‚„ã™
- ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´
- ä¸è¦ãªãƒ¢ãƒ‡ãƒ«ã¯ç„¡åŠ¹åŒ–

## ğŸ¤ è²¢çŒ®

1. Fork this repository
2. Create your feature branch
3. Commit your changes  
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ™ è¬è¾

- Hugging Face Transformers
- FAISS by Facebook Research
- Gradio team
- Wikipedia contributors
\\`\\`\\`

## å—ã‘å…¥ã‚ŒåŸºæº–
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆãŒå‹•ä½œã™ã‚‹
- [ ] ä¸»è¦ãªæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸â‰¥80%
- [ ] ç™ºè¦‹ã•ã‚ŒãŸãƒã‚°ãŒä¿®æ­£ã•ã‚Œã‚‹
- [ ] README.mdãŒåŒ…æ‹¬çš„ã«æ›´æ–°ã•ã‚Œã‚‹
- [ ] ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰ãŒä½œæˆã•ã‚Œã‚‹
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒå®Ÿè¡Œã§ãã‚‹

## å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ
- å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆãƒ†ã‚¹ãƒˆè¨­è¨ˆ
- ãƒ‡ãƒãƒƒã‚°æŠ€æ³•ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- æŠ€è¡“æ–‡æ›¸ä½œæˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ã‚’è€ƒæ…®ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- ç¶™ç¶šçš„å“è³ªæ”¹å–„ã®ãƒ—ãƒ­ã‚»ã‚¹

## ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ–¹æ³•
```bash
# çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python -m pytest tests/test_integration.py -v

# ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š
pip install pytest-cov
python -m pytest --cov=src tests/

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
python tests/test_integration.py::TestIntegration::test_performance_benchmarks
```

## å®Œäº†å¾Œã®ç¢ºèªäº‹é …
- [ ] å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒé€šéã™ã‚‹
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒæœ€æ–°ã§æ­£ç¢º
- [ ] ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ãŒé©åˆ‡ã«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã•ã‚Œã‚‹
- [ ] ãƒ¦ãƒ¼ã‚¶ã‚¬ã‚¤ãƒ‰ãŒåˆ†ã‹ã‚Šã‚„ã™ã„
- [ ] ã‚·ã‚¹ãƒ†ãƒ ãŒå®‰å®šã—ã¦å‹•ä½œã™ã‚‹

ã“ã‚Œã§ã€å­¦ç¿’ç”¨Wikipedia RAGã‚·ã‚¹ãƒ†ãƒ ã®å®Œå…¨ãªå®Ÿè£…ã¨æ¤œè¨¼ãŒå®Œäº†ã—ã¾ã™ï¼