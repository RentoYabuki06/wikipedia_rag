# Issue #11: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆã¨Recall@Kè©•ä¾¡æ©Ÿèƒ½ã®å®Ÿè£…

**äºˆæƒ³æ™‚é–“**: 25åˆ†
**é›£æ˜“åº¦**: ä¸­ç´š
**å­¦ç¿’é …ç›®**: æƒ…å ±æ¤œç´¢è©•ä¾¡ã€Recall@Kã€è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­è¨ˆ

## æ¦‚è¦
RAGã‚·ã‚¹ãƒ†ãƒ ã®æ¤œç´¢æ€§èƒ½ã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹ãŸã‚ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã€Recall@KæŒ‡æ¨™ã‚’è¨ˆç®—ã™ã‚‹æ©Ÿèƒ½ã‚’å®Ÿè£…ã™ã‚‹ã€‚

## ã‚¿ã‚¹ã‚¯

### 1. è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ (`data/dev.jsonl`)
- æ—¥æœ¬èªã®è³ªå•ã¨æ­£è§£ã‚½ãƒ¼ã‚¹ã®ãƒšã‚¢ä½œæˆ
- Wikipediaè¨˜äº‹ã«åŸºã¥ã„ãŸå®Ÿåœ¨ã™ã‚‹æƒ…å ±ã§ã®è³ªå•è¨­è¨ˆ
- å¤šæ§˜ãªè³ªå•ã‚¿ã‚¤ãƒ—ï¼ˆäººç‰©ã€å‡ºæ¥äº‹ã€æ¦‚å¿µãªã©ï¼‰ã®åŒ…æ‹¬

### 2. è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè£… (`src/eval_retrieval.py`)
- Recall@Kè¨ˆç®—æ©Ÿèƒ½
- è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ï¼ˆRecall@1, @3, @5ï¼‰
- è©³ç´°ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### 3. è©•ä¾¡ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®å®šç¾©
```json
{
    "question": "è³ªå•æ–‡",
    "gold_sources": ["jawiki:è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«#chunk=ç•ªå·", ...],
    "category": "äººç‰©|å‡ºæ¥äº‹|æ¦‚å¿µ|ãã®ä»–",
    "difficulty": "easy|medium|hard"
}
```

### 4. è‡ªå‹•è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã§ã®æ€§èƒ½æ¸¬å®š
- ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ»é›£æ˜“åº¦åˆ¥ã®åˆ†æ
- å¤±æ•—ã‚±ãƒ¼ã‚¹ã®è©³ç´°åˆ†æ

## å®Ÿè£…ä¾‹ã®ã‚¹ã‚±ãƒ«ãƒˆãƒ³

### è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µãƒ³ãƒ—ãƒ« (`data/dev.jsonl`)
```jsonl
{"question": "ç¹”ç”°ä¿¡é•·ãŒæœ¬èƒ½å¯ºã®å¤‰ã§äº¡ããªã£ãŸã®ã¯ä½•å¹´ã§ã™ã‹ï¼Ÿ", "gold_sources": ["jawiki:ç¹”ç”°ä¿¡é•·#chunk=5", "jawiki:æœ¬èƒ½å¯ºã®å¤‰#chunk=1"], "category": "äººç‰©", "difficulty": "easy"}
{"question": "å¤§æ”¿å¥‰é‚„ãŒè¡Œã‚ã‚ŒãŸå…·ä½“çš„ãªæ—¥ä»˜ã‚’æ•™ãˆã¦ãã ã•ã„", "gold_sources": ["jawiki:å¤§æ”¿å¥‰é‚„#chunk=1"], "category": "å‡ºæ¥äº‹", "difficulty": "medium"}
{"question": "æ±Ÿæˆ¸å¹•åºœã®æˆç«‹ã‹ã‚‰çµ‚ã‚ã‚Šã¾ã§ã®æœŸé–“ã¯ã©ã®ãã‚‰ã„ã§ã™ã‹ï¼Ÿ", "gold_sources": ["jawiki:æ±Ÿæˆ¸å¹•åºœ#chunk=1", "jawiki:æ±Ÿæˆ¸æ™‚ä»£#chunk=0"], "category": "æ¦‚å¿µ", "difficulty": "medium"}
{"question": "å‚æœ¬é¾é¦¬ãŒè–©é•·åŒç›Ÿã®ä»²ä»‹ã§æœãŸã—ãŸå…·ä½“çš„ãªå½¹å‰²ã«ã¤ã„ã¦", "gold_sources": ["jawiki:å‚æœ¬é¾é¦¬#chunk=3", "jawiki:è–©é•·åŒç›Ÿ#chunk=2"], "category": "äººç‰©", "difficulty": "hard"}
{"question": "æ˜æ²»ç¶­æ–°ã«ãŠã‘ã‚‹å»ƒè—©ç½®çœŒã®ç›®çš„ã¨åŠ¹æœã‚’èª¬æ˜ã—ã¦ãã ã•ã„", "gold_sources": ["jawiki:å»ƒè—©ç½®çœŒ#chunk=0", "jawiki:æ˜æ²»ç¶­æ–°#chunk=4"], "category": "å‡ºæ¥äº‹", "difficulty": "hard"}
```

### è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (`src/eval_retrieval.py`)
```python
import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple
from collections import defaultdict

from rag_wiki import WikiRAG

class RetrievalEvaluator:
    def __init__(self, rag_system: WikiRAG):
        """æ¤œç´¢è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–"""
        self.rag_system = rag_system
        
    def load_evaluation_data(self, filepath: str) -> List[Dict[str, Any]]:
        """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        eval_data = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                eval_data.append(json.loads(line.strip()))
        return eval_data
    
    def evaluate_single_question(self, question: str, gold_sources: List[str], 
                               top_k: int = 16) -> Dict[str, Any]:
        """å˜ä¸€è³ªå•ã®è©•ä¾¡"""
        try:
            # æ¤œç´¢å®Ÿè¡Œï¼ˆç”Ÿæˆã¯è¡Œã‚ãšã€æ¤œç´¢çµæœã®ã¿å–å¾—ï¼‰
            query_embedding = self.rag_system.embedder.encode_query(question)
            scores, indices = self.rag_system.vector_store.search(query_embedding, k=top_k)
            
            # æ¤œç´¢çµæœã®å–å¾—
            retrieved_sources = []
            for idx in indices[0]:
                if idx < len(self.rag_system.metadata):
                    metadata = self.rag_system.metadata[idx]
                    source = f"jawiki:{metadata['article_title']}#chunk={metadata['chunk_id']}"
                    retrieved_sources.append(source)
            
            # Recall@Kè¨ˆç®—
            recall_metrics = {}
            for k in [1, 3, 5, 10]:
                if k <= len(retrieved_sources):
                    top_k_retrieved = set(retrieved_sources[:k])
                    gold_set = set(gold_sources)
                    recall = len(top_k_retrieved.intersection(gold_set)) / len(gold_set) if gold_set else 0.0
                    recall_metrics[f'recall@{k}'] = recall
                else:
                    recall_metrics[f'recall@{k}'] = 0.0
            
            return {
                'retrieved_sources': retrieved_sources,
                'gold_sources': gold_sources,
                'metrics': recall_metrics,
                'success': True
            }
            
        except Exception as e:
            logging.error(f"Evaluation error for question: {question[:50]}... - {e}")
            return {
                'retrieved_sources': [],
                'gold_sources': gold_sources,
                'metrics': {f'recall@{k}': 0.0 for k in [1, 3, 5, 10]},
                'success': False,
                'error': str(e)
            }
    
    def evaluate_dataset(self, eval_data: List[Dict[str, Any]], top_k: int = 16) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®è©•ä¾¡"""
        all_metrics = defaultdict(list)
        category_metrics = defaultdict(lambda: defaultdict(list))
        difficulty_metrics = defaultdict(lambda: defaultdict(list))
        failed_questions = []
        
        logging.info(f"Evaluating {len(eval_data)} questions...")
        
        for i, item in enumerate(eval_data):
            if (i + 1) % 10 == 0:
                logging.info(f"Processed {i + 1}/{len(eval_data)} questions")
            
            question = item['question']
            gold_sources = item['gold_sources']
            category = item.get('category', 'unknown')
            difficulty = item.get('difficulty', 'unknown')
            
            result = self.evaluate_single_question(question, gold_sources, top_k)
            
            if result['success']:
                # å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                for metric_name, value in result['metrics'].items():
                    all_metrics[metric_name].append(value)
                
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                for metric_name, value in result['metrics'].items():
                    category_metrics[category][metric_name].append(value)
                
                # é›£æ˜“åº¦åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                for metric_name, value in result['metrics'].items():
                    difficulty_metrics[difficulty][metric_name].append(value)
            else:
                failed_questions.append({
                    'question': question,
                    'error': result.get('error', 'Unknown error')
                })
        
        # å¹³å‡å€¤è¨ˆç®—
        def calculate_averages(metrics_dict):
            return {k: sum(v) / len(v) if v else 0.0 for k, v in metrics_dict.items()}
        
        results = {
            'overall_metrics': calculate_averages(all_metrics),
            'category_metrics': {cat: calculate_averages(metrics) 
                               for cat, metrics in category_metrics.items()},
            'difficulty_metrics': {diff: calculate_averages(metrics) 
                                 for diff, metrics in difficulty_metrics.items()},
            'total_questions': len(eval_data),
            'successful_evaluations': len(eval_data) - len(failed_questions),
            'failed_questions': failed_questions
        }
        
        return results
    
    def print_evaluation_report(self, results: Dict[str, Any]):
        """è©•ä¾¡çµæœãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›"""
        print("\\n" + "=" * 60)
        print("RAG ã‚·ã‚¹ãƒ†ãƒ  æ¤œç´¢æ€§èƒ½è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 60)
        
        # å…¨ä½“çµæœ
        print(f"\\nğŸ“Š å…¨ä½“çµæœ ({results['successful_evaluations']}/{results['total_questions']} ä»¶æˆåŠŸ)")
        for metric, value in results['overall_metrics'].items():
            print(f"  {metric.upper()}: {value:.3f}")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ
        if results['category_metrics']:
            print(f"\\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ:")
            for category, metrics in results['category_metrics'].items():
                print(f"  {category}:")
                for metric, value in metrics.items():
                    print(f"    {metric}: {value:.3f}")
        
        # é›£æ˜“åº¦åˆ¥çµæœ
        if results['difficulty_metrics']:
            print(f"\\nâ­ é›£æ˜“åº¦åˆ¥çµæœ:")
            for difficulty, metrics in results['difficulty_metrics'].items():
                print(f"  {difficulty}:")
                for metric, value in metrics.items():
                    print(f"    {metric}: {value:.3f}")
        
        # å¤±æ•—ã‚±ãƒ¼ã‚¹
        if results['failed_questions']:
            print(f"\\nâŒ å¤±æ•—ã‚±ãƒ¼ã‚¹ ({len(results['failed_questions'])} ä»¶):")
            for i, failed in enumerate(results['failed_questions'][:5]):  # æœ€å¤§5ä»¶è¡¨ç¤º
                print(f"  {i+1}. {failed['question'][:50]}...")
                print(f"     ã‚¨ãƒ©ãƒ¼: {failed['error']}")
        
        print("=" * 60)

def parse_arguments():
    parser = argparse.ArgumentParser(description="RAGæ¤œç´¢æ€§èƒ½è©•ä¾¡")
    parser.add_argument("--eval_data", default="data/dev.jsonl", help="è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--artifacts_dir", default="artifacts", help="ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--top_k", type=int, default=16, help="æ¤œç´¢æ™‚ã®top-k")
    parser.add_argument("--output", help="çµæœã‚’JSONã§ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å")
    return parser.parse_args()

def main():
    args = parse_arguments()
    
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    try:
        # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        logging.info("Initializing RAG system...")
        rag_system = WikiRAG(args.artifacts_dir)
        
        # è©•ä¾¡å™¨åˆæœŸåŒ–
        evaluator = RetrievalEvaluator(rag_system)
        
        # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if not Path(args.eval_data).exists():
            logging.error(f"Evaluation data file not found: {args.eval_data}")
            return
        
        eval_data = evaluator.load_evaluation_data(args.eval_data)
        logging.info(f"Loaded {len(eval_data)} evaluation questions")
        
        # è©•ä¾¡å®Ÿè¡Œ
        results = evaluator.evaluate_dataset(eval_data, args.top_k)
        
        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        evaluator.print_evaluation_report(results)
        
        # JSONä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logging.info(f"Results saved to {args.output}")
        
    except Exception as e:
        logging.error(f"Evaluation failed: {e}")
        raise

if __name__ == "__main__":
    main()
```

## å—ã‘å…¥ã‚ŒåŸºæº–
- [ ] è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ20å•ä»¥ä¸Šï¼‰ãŒä½œæˆã•ã‚Œã‚‹
- [ ] Recall@Kè¨ˆç®—ãŒæ­£ç¢ºã«å‹•ä½œã™ã‚‹
- [ ] ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ»é›£æ˜“åº¦åˆ¥ã®åˆ†æãŒæ©Ÿèƒ½ã™ã‚‹
- [ ] è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆãŒè¦‹ã‚„ã™ã„å½¢å¼ã§å‡ºåŠ›ã•ã‚Œã‚‹
- [ ] å¤±æ•—ã‚±ãƒ¼ã‚¹ãŒé©åˆ‡ã«è¨˜éŒ²ãƒ»è¡¨ç¤ºã•ã‚Œã‚‹
- [ ] JSONã§ã®çµæœä¿å­˜æ©Ÿèƒ½ãŒå‹•ä½œã™ã‚‹

## å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ
- æƒ…å ±æ¤œç´¢ã«ãŠã‘ã‚‹è©•ä¾¡æŒ‡æ¨™ã®ç†è§£
- Recall@Kã®è¨ˆç®—æ–¹æ³•ã¨æ„å‘³
- è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­è¨ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ã®å®¢è¦³çš„æ¸¬å®šæ–¹æ³•
- è©•ä¾¡çµæœã®åŠ¹æœçš„ãªå¯è¦–åŒ–

## ãƒ†ã‚¹ãƒˆæ–¹æ³•
```bash
# è©•ä¾¡å®Ÿè¡Œ
python src/eval_retrieval.py --eval_data data/dev.jsonl

# çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
python src/eval_retrieval.py --output evaluation_results.json

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ã®è©•ä¾¡
python src/eval_retrieval.py --top_k 20 --artifacts_dir artifacts
```

## æœŸå¾…ã•ã‚Œã‚‹çµæœ
- Recall@5 â‰¥ 0.6ï¼ˆæš«å®šåŸºæº–ï¼‰
- ã‚«ãƒ†ã‚´ãƒªé–“ã§ã®æ€§èƒ½å·®ã®æŠŠæ¡
- æ”¹å–„ãŒå¿…è¦ãªé ˜åŸŸã®ç‰¹å®š

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
Issue #12: çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™